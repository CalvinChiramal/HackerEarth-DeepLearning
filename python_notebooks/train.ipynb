{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OljNN_nhu5tW",
        "colab_type": "code",
        "outputId": "f3d65fb3-8a40-453d-ab4d-5507f4d0a12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!git clone https://github.com/Calvinwilson99/HackerEarth-DeepLearning.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'HackerEarth-DeepLearning'...\n",
            "remote: Enumerating objects: 9128, done.\u001b[K\n",
            "remote: Total 9128 (delta 0), reused 0 (delta 0), pack-reused 9128\u001b[K\n",
            "Receiving objects: 100% (9128/9128), 74.18 MiB | 34.62 MiB/s, done.\n",
            "Resolving deltas: 100% (1927/1927), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEJXYJFvwMWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "05393991-0d85-4a43-8dd1-a1fdb50cbf04"
      },
      "source": [
        "# Import necessary header files \n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import VGG16\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVfNIa4Twqld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the training dataset\n",
        "# y is list of target values\n",
        "\n",
        "train = pd.read_csv(\"/content/HackerEarth-DeepLearning/dataset/train.csv\")\n",
        "y = train.iloc[:,1].values\n",
        "y = LabelEncoder().fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9NQwzhlw3lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read each image and add matrix to X (list of training values)\n",
        "\n",
        "X = []\n",
        "for i in range(len(train)):\n",
        "    image = cv2.imread(\"/content/HackerEarth-DeepLearning/dataset/Train Images/\" + train.Image[i])\n",
        "    resized = cv2.resize(image, (224,224))\n",
        "    X.append(resized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzckiMQ4zZ94",
        "colab_type": "code",
        "outputId": "326a57ff-9702-476e-8c5b-6be9a8778de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Split into training and test set\n",
        "\n",
        "X = np.array(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "print(X_train.dtype)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPG6yPCSzrEo",
        "colab_type": "code",
        "outputId": "27018789-a2ec-4a9e-bab6-bb3604f1f987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# Import VGG16 architecture to help in learning - Expects input shape to be (224,224,3) (remove output layer)\n",
        "# Add our final layer for output\n",
        "\n",
        "trained_model = VGG16(weights=\"imagenet\",\n",
        "    include_top=False, \n",
        "    input_shape=(224, 224, 3), \n",
        "    pooling='avg')\n",
        "trained_model.trainable = False\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(trained_model)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(4, activation = \"softmax\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnD9NAGE0AAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# callbacks to save model weights at checkpoints, change learning rate dynamically\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience = 10, verbose = 1),\n",
        "    ReduceLROnPlateau(factor = 0.1, patience = 3,\n",
        "    min_lr = 0.00001, verbose = 1),\n",
        "    ModelCheckpoint('/content/HackerEarth-DeepLearning/models/model.h5',verbose = 1, save_best_only = True,\n",
        "    save_weights_only = True)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECXnUbH90SxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer = \"Adam\", metrics = ['accuracy'], loss = 'sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymj97XG90U79",
        "colab_type": "code",
        "outputId": "9412e306-b71b-4875-9af6-51f906c0761d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(X_train, y_train, epochs = 50, validation_data = (X_test,y_test), callbacks = callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4786 samples, validate on 1197 samples\n",
            "Epoch 1/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 3.2261 - acc: 0.4987\n",
            "Epoch 00001: val_loss improved from inf to 1.77396, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 40s 8ms/sample - loss: 3.2227 - acc: 0.4992 - val_loss: 1.7740 - val_acc: 0.6374\n",
            "Epoch 2/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 1.7305 - acc: 0.6391\n",
            "Epoch 00002: val_loss improved from 1.77396 to 1.18174, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 1.7287 - acc: 0.6396 - val_loss: 1.1817 - val_acc: 0.6892\n",
            "Epoch 3/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 1.2511 - acc: 0.6831\n",
            "Epoch 00003: val_loss improved from 1.18174 to 1.00108, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 29s 6ms/sample - loss: 1.2502 - acc: 0.6828 - val_loss: 1.0011 - val_acc: 0.7185\n",
            "Epoch 4/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 1.0169 - acc: 0.7114\n",
            "Epoch 00004: val_loss improved from 1.00108 to 0.96883, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 1.0156 - acc: 0.7112 - val_loss: 0.9688 - val_acc: 0.7352\n",
            "Epoch 5/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.8843 - acc: 0.7311\n",
            "Epoch 00005: val_loss improved from 0.96883 to 0.84928, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.8861 - acc: 0.7309 - val_loss: 0.8493 - val_acc: 0.7527\n",
            "Epoch 6/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.8334 - acc: 0.7324\n",
            "Epoch 00006: val_loss improved from 0.84928 to 0.79709, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.8316 - acc: 0.7328 - val_loss: 0.7971 - val_acc: 0.7485\n",
            "Epoch 7/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.7433\n",
            "Epoch 00007: val_loss improved from 0.79709 to 0.75873, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.7624 - acc: 0.7430 - val_loss: 0.7587 - val_acc: 0.7310\n",
            "Epoch 8/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.7191 - acc: 0.7534\n",
            "Epoch 00008: val_loss did not improve from 0.75873\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.7189 - acc: 0.7534 - val_loss: 0.7652 - val_acc: 0.7335\n",
            "Epoch 9/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.6871 - acc: 0.7546\n",
            "Epoch 00009: val_loss improved from 0.75873 to 0.74459, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.6862 - acc: 0.7551 - val_loss: 0.7446 - val_acc: 0.7452\n",
            "Epoch 10/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.6839 - acc: 0.7527\n",
            "Epoch 00010: val_loss improved from 0.74459 to 0.69883, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.6830 - acc: 0.7528 - val_loss: 0.6988 - val_acc: 0.7527\n",
            "Epoch 11/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.7592\n",
            "Epoch 00011: val_loss improved from 0.69883 to 0.69826, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.6569 - acc: 0.7591 - val_loss: 0.6983 - val_acc: 0.7694\n",
            "Epoch 12/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.6535 - acc: 0.7588\n",
            "Epoch 00012: val_loss did not improve from 0.69826\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.6533 - acc: 0.7587 - val_loss: 0.7195 - val_acc: 0.7569\n",
            "Epoch 13/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.6460 - acc: 0.7638\n",
            "Epoch 00013: val_loss did not improve from 0.69826\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.6454 - acc: 0.7639 - val_loss: 0.7356 - val_acc: 0.7419\n",
            "Epoch 14/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.6565 - acc: 0.7626\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69826\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.6556 - acc: 0.7628 - val_loss: 0.7701 - val_acc: 0.7586\n",
            "Epoch 15/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7890\n",
            "Epoch 00015: val_loss improved from 0.69826 to 0.69127, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5752 - acc: 0.7888 - val_loss: 0.6913 - val_acc: 0.7586\n",
            "Epoch 16/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5749 - acc: 0.7842\n",
            "Epoch 00016: val_loss improved from 0.69127 to 0.68131, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5740 - acc: 0.7846 - val_loss: 0.6813 - val_acc: 0.7619\n",
            "Epoch 17/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5681 - acc: 0.7846\n",
            "Epoch 00017: val_loss improved from 0.68131 to 0.67785, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5666 - acc: 0.7852 - val_loss: 0.6779 - val_acc: 0.7594\n",
            "Epoch 18/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.7930\n",
            "Epoch 00018: val_loss improved from 0.67785 to 0.67512, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5564 - acc: 0.7925 - val_loss: 0.6751 - val_acc: 0.7611\n",
            "Epoch 19/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7846\n",
            "Epoch 00019: val_loss did not improve from 0.67512\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5608 - acc: 0.7842 - val_loss: 0.6781 - val_acc: 0.7569\n",
            "Epoch 20/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.7878\n",
            "Epoch 00020: val_loss improved from 0.67512 to 0.67146, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5608 - acc: 0.7873 - val_loss: 0.6715 - val_acc: 0.7577\n",
            "Epoch 21/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.7880\n",
            "Epoch 00021: val_loss did not improve from 0.67146\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5545 - acc: 0.7877 - val_loss: 0.6728 - val_acc: 0.7527\n",
            "Epoch 22/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.7886\n",
            "Epoch 00022: val_loss improved from 0.67146 to 0.66708, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5543 - acc: 0.7888 - val_loss: 0.6671 - val_acc: 0.7586\n",
            "Epoch 23/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5587 - acc: 0.7884\n",
            "Epoch 00023: val_loss improved from 0.66708 to 0.66326, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5580 - acc: 0.7885 - val_loss: 0.6633 - val_acc: 0.7586\n",
            "Epoch 24/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7972\n",
            "Epoch 00024: val_loss did not improve from 0.66326\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5402 - acc: 0.7975 - val_loss: 0.6672 - val_acc: 0.7519\n",
            "Epoch 25/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.7919\n",
            "Epoch 00025: val_loss improved from 0.66326 to 0.66061, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5577 - acc: 0.7919 - val_loss: 0.6606 - val_acc: 0.7619\n",
            "Epoch 26/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.7869\n",
            "Epoch 00026: val_loss improved from 0.66061 to 0.65831, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5550 - acc: 0.7867 - val_loss: 0.6583 - val_acc: 0.7569\n",
            "Epoch 27/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5564 - acc: 0.7854\n",
            "Epoch 00027: val_loss improved from 0.65831 to 0.65815, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5562 - acc: 0.7856 - val_loss: 0.6581 - val_acc: 0.7644\n",
            "Epoch 28/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5480 - acc: 0.7964\n",
            "Epoch 00028: val_loss improved from 0.65815 to 0.65639, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5485 - acc: 0.7963 - val_loss: 0.6564 - val_acc: 0.7577\n",
            "Epoch 29/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.7924\n",
            "Epoch 00029: val_loss did not improve from 0.65639\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5411 - acc: 0.7923 - val_loss: 0.6638 - val_acc: 0.7661\n",
            "Epoch 30/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7880\n",
            "Epoch 00030: val_loss did not improve from 0.65639\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5387 - acc: 0.7879 - val_loss: 0.6572 - val_acc: 0.7627\n",
            "Epoch 31/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.7892\n",
            "Epoch 00031: val_loss improved from 0.65639 to 0.65549, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5472 - acc: 0.7892 - val_loss: 0.6555 - val_acc: 0.7652\n",
            "Epoch 32/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.7936\n",
            "Epoch 00032: val_loss improved from 0.65549 to 0.65380, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5454 - acc: 0.7936 - val_loss: 0.6538 - val_acc: 0.7644\n",
            "Epoch 33/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.7957\n",
            "Epoch 00033: val_loss improved from 0.65380 to 0.64951, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5492 - acc: 0.7965 - val_loss: 0.6495 - val_acc: 0.7652\n",
            "Epoch 34/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.7888\n",
            "Epoch 00034: val_loss did not improve from 0.64951\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5584 - acc: 0.7885 - val_loss: 0.6525 - val_acc: 0.7661\n",
            "Epoch 35/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.7892\n",
            "Epoch 00035: val_loss improved from 0.64951 to 0.64760, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5486 - acc: 0.7894 - val_loss: 0.6476 - val_acc: 0.7594\n",
            "Epoch 36/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.7905\n",
            "Epoch 00036: val_loss did not improve from 0.64760\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5482 - acc: 0.7906 - val_loss: 0.6486 - val_acc: 0.7661\n",
            "Epoch 37/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5337 - acc: 0.7934\n",
            "Epoch 00037: val_loss improved from 0.64760 to 0.64754, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5368 - acc: 0.7919 - val_loss: 0.6475 - val_acc: 0.7694\n",
            "Epoch 38/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5536 - acc: 0.7842\n",
            "Epoch 00038: val_loss improved from 0.64754 to 0.64616, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5541 - acc: 0.7837 - val_loss: 0.6462 - val_acc: 0.7686\n",
            "Epoch 39/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.7938\n",
            "Epoch 00039: val_loss did not improve from 0.64616\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5416 - acc: 0.7938 - val_loss: 0.6464 - val_acc: 0.7678\n",
            "Epoch 40/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.7972\n",
            "Epoch 00040: val_loss improved from 0.64616 to 0.64494, saving model to /content/HackerEarth-DeepLearning/model.h5\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5366 - acc: 0.7973 - val_loss: 0.6449 - val_acc: 0.7678\n",
            "Epoch 41/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7884\n",
            "Epoch 00041: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5459 - acc: 0.7883 - val_loss: 0.6472 - val_acc: 0.7602\n",
            "Epoch 42/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.7878\n",
            "Epoch 00042: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5454 - acc: 0.7873 - val_loss: 0.6499 - val_acc: 0.7728\n",
            "Epoch 43/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.7894\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5501 - acc: 0.7894 - val_loss: 0.6456 - val_acc: 0.7703\n",
            "Epoch 44/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.7974\n",
            "Epoch 00044: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5301 - acc: 0.7977 - val_loss: 0.6461 - val_acc: 0.7686\n",
            "Epoch 45/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8005\n",
            "Epoch 00045: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5242 - acc: 0.8009 - val_loss: 0.6463 - val_acc: 0.7669\n",
            "Epoch 46/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7995\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5320 - acc: 0.7998 - val_loss: 0.6453 - val_acc: 0.7694\n",
            "Epoch 47/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.7974\n",
            "Epoch 00047: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5338 - acc: 0.7975 - val_loss: 0.6456 - val_acc: 0.7694\n",
            "Epoch 48/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.7959\n",
            "Epoch 00048: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5231 - acc: 0.7961 - val_loss: 0.6449 - val_acc: 0.7686\n",
            "Epoch 49/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.7980\n",
            "Epoch 00049: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5269 - acc: 0.7977 - val_loss: 0.6457 - val_acc: 0.7678\n",
            "Epoch 50/50\n",
            "4768/4786 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.7955\n",
            "Epoch 00050: val_loss did not improve from 0.64494\n",
            "4786/4786 [==============================] - 28s 6ms/sample - loss: 0.5369 - acc: 0.7957 - val_loss: 0.6452 - val_acc: 0.7694\n",
            "Epoch 00050: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f35f007f240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ThbrJnqUkCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load saved weights for prediction\n",
        "\n",
        "pred_model = Sequential()\n",
        "\n",
        "pred_model.add(trained_model)\n",
        "pred_model.add(Dropout(0.2))\n",
        "pred_model.add(Dense(4, activation = \"softmax\"))\n",
        "pred_model.load_weights('/content/HackerEarth-DeepLearning/models/best_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7ck-ghg3dqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting output on X_test\n",
        "\n",
        "y_pred = pred_model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXbBsRmQATdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COnverting output to text labels\n",
        "\n",
        "y_pre = [np.argmax(i) for i in y_pred]\n",
        "output = [\"Attire\", \"Decorationandsignage\", \"Food\", \"misc\"]\n",
        "\n",
        "pred_labels = [output[i] for i in y_pre]\n",
        "corr_labels = [output[i] for i in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGQ3RGgqBo-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "c156df17-362b-4c07-cd8d-f646702725f2"
      },
      "source": [
        "# Print accuracy\n",
        "\n",
        "print(\"ACCURACY SCORE\")\n",
        "print(accuracy_score(y_test, y_pre))\n",
        "\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(confusion_matrix(y_test, y_pre))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE\n",
            "0.7677527151211362\n",
            "CONFUSION MATRIX\n",
            "[[259   4  37  30]\n",
            " [  6 114  12  11]\n",
            " [ 38  18 388  16]\n",
            " [ 37  19  50 158]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7BIFRp43gjt",
        "colab_type": "code",
        "outputId": "b8bd9133-9ed9-40bc-8a32-a3cf474c465f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# testing the results (run cell again for different outputs)\n",
        "\n",
        "for i in range(4):\n",
        "  ind = random.randint(0, len(y_test))\n",
        "  print(\"predicted: \", pred_labels[ind], \"Correct: \", corr_labels[ind])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted:  Food Correct:  Food\n",
            "predicted:  Food Correct:  misc\n",
            "predicted:  Food Correct:  Food\n",
            "predicted:  misc Correct:  misc\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}